{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Recurrent Neural Network (LSTM) using Python\n",
    "\n",
    "### Major tasks we need to do for implementing RNN (LSTM):\n",
    "    ~ Task 1 - Data Preprocessing\n",
    "    ~ Task 2 - Building the Recurrent Neural Network\n",
    "    ~ Task 3 - Making Predictions and Visualizing Stuff\n",
    "    \n",
    "### Before implementing, setting up the environment by installing relevant libraries:\n",
    "    ~ python -m pip install --user numpy scipy matplotlib ipython jupyter pandas sympy nose\n",
    "    ~ pip install scikit-learn\n",
    "    ~ pip install tensorflow\n",
    "    ~ pip install keras\n",
    "\n",
    "### Let us start the implementation of RNN (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1 - Data Preprocessing\n",
    "\n",
    "# importing the libraries\n",
    "# numpy arrays are only allowed as inputs to neural networks in keras and not the pandas data-frames\n",
    "import numpy as np\n",
    "# to visualize the results\n",
    "import matplotlib.pyplot as plt\n",
    "# for importing and managing the dataset easily\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the training set\n",
    "dataset_train = pd.read_csv(\"Google_Stock_Price_Train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we select the right column from the \"dataset_train\" dataset for making the \"training_set\"\n",
    "training_set = dataset_train.iloc[:, 1:2].values\n",
    "# building the the training set numpy array on which the RNN will be trained; we only choose the \"Open\" column i.e. column 1\n",
    "# note: if in the above line, we would not have added \".values\", then it would just have been a data-frame\n",
    "# note that iloc method is used to get the right index of the desired columns\n",
    "# the first parameter in the iloc is the row selector parameter and the second parameter is the column selector parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature scaling\n",
    "# feature scaling can be done through standardisation or normalization\n",
    "# here we use normalization\n",
    "# note that it is recommended that whenever the sigmoid activation function is involved, use normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# create the MinMaxScaler object\n",
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "# (0, 1) because we want the scaling to be done within this limit\n",
    "training_set_scaled = sc.fit_transform(training_set)\n",
    "# fit_transform fits and scales/transforms the data\n",
    "# note: fit means it will calculate the min() and max() of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a data structure with 60 timesteps and 1 output\n",
    "# this means, the RNN will take into account the previous 60 records to predict one output at a particular time t\n",
    "# this may be called as the Backpropagation Through Time (BPTT)\n",
    "# we need to use this concept to avoid over-fitting or wrong predictions\n",
    "# we are now going to form two entities: X_train and y_train\n",
    "# X_train will be the input to the neural network\n",
    "# y_train will be the output to the neural network\n",
    "# for any financial day X_train will have the 60 records of the previous 60 stock prices\n",
    "# y_train will have the stock price of the next financial day\n",
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(60, 1258):\n",
    "    X_train.append(training_set_scaled[i - 60 : i, 0])\n",
    "    # 0 means the column 0\n",
    "    # note that RNN is memorizing at this stage\n",
    "    y_train.append(training_set_scaled[i : i + 1, 0])\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "# reshaping the data - the last step in data preprocessing\n",
    "# we will be adding a new dimension to the data set\n",
    "# this will help in better predictions\n",
    "# for adding a new dimension to the numpy array, always use the reshape function\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "# the second parameter takes in the batch size (rows), timesteps (columns), and the number of indicatiors/predictors\n",
    "# in current set (1) - Open stock price column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2 - Building the RNN\n",
    "\n",
    "# importing the Keras libraires and packages\n",
    "from keras.models import Sequential # building the sequence of layers\n",
    "from keras.layers import Dense # building the hidden layes\n",
    "from keras.layers import LSTM # building the LSTM layers\n",
    "from keras.layers import Dropout # for Dropout Regularization - avoiding over-fitting\n",
    "\n",
    "# initializing the RNN\n",
    "regressor = Sequential() # regressor because we are predicting a continuous output\n",
    "\n",
    "# adding the first LSTM layer and Droupout Regularizaiton\n",
    "regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
    "# first parameter is the number of LSTM memory units/cells or neurons, second parameter is the return sequence \n",
    "# which will be True as we are making a stacked LSTM, the third argument is the input shape\n",
    "# in third parameter, we will add the last two shapes from X_train as 0th will be added automatically\n",
    "regressor.add(Dropout(0.20)) # mention the dropout rate for neurons...10 neurons will dropout at each iteration...0.20 * 50 = 10\n",
    "\n",
    "# adding the second LSTM layer and Droupout Regularizaiton\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "# first parameter is the number of LSTM memory units/cells or neurons, second parameter is the return sequence \n",
    "# which will be True as we are making a stacked LSTM, the third argument is the input shape\n",
    "# in third parameter, we will add the last two shapes from X_train as 0th will be added automatically\n",
    "regressor.add(Dropout(0.20)) # mention the dropout rate for neurons...10 neurons will dropout at each iteration...0.20 * 50 = 10\n",
    "\n",
    "# adding the third LSTM layer and Droupout Regularizaiton\n",
    "regressor.add(LSTM(units = 50, return_sequences = True)) # first parameter is the number of LSTM memory\n",
    "# units/cells or neurons, second parameter is the return sequence which will be True as we are making a stacked LSTM,\n",
    "# the third argument is the input shape\n",
    "# in third parameter, we will add the last two shapes from X_train as 0th will be added automatically\n",
    "regressor.add(Dropout(0.20)) # mention the dropout rate for neurons...10 neurons will dropout at each iteration...0.20 * 50 = 10\n",
    "\n",
    "# adding the fourth LSTM layer and Droupout Regularizaiton\n",
    "regressor.add(LSTM(units = 50)) # first parameter is the number of LSTM memory units/cells or neurons,\n",
    "# second parameter is the return sequence which will be True as we are making a stacked LSTM,\n",
    "# the third argument is the input shape\n",
    "# in third parameter, we will add the last two shapes from X_train as 0th will be added automatically\n",
    "regressor.add(Dropout(0.20)) # mention the dropout rate for neurons...10 neurons will dropout at each iteration...0.20 * 50 = 10\n",
    "\n",
    "# adding the last LSTM layer and Droupout Regularizaiton\n",
    "regressor.add(Dense(units = 1)) # the parameter is for specifying the output neurons\n",
    "\n",
    "# compiling the RNN\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "# rmsprop optimizer is generally used in RNN but adam is too strong\n",
    "# for classification problem, loss should be say binary_crossentropy\n",
    "\n",
    "# fitting the RNN to the training set\n",
    "regressor.fit(X_train, y_train, epochs = 100, batch_size = 32)\n",
    "# epochs mean how many times the data should be forward propagated and back propagated or say\n",
    "# show many times the neural network should be trained\n",
    "# the last parameter specifies the batch size for training - how many records should be trained at a single time\n",
    "# the training will take some time - around 15 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3 - predicting results and generating visualizaitons\n",
    "\n",
    "dataset_test = pd.read_csv(\"Google_Stock_Price_Test.csv\")\n",
    "real_test_set = dataset_test.iloc[:, 1:2].values\n",
    "\n",
    "# note: for predicting the Jan 2017 stock prices, we need previous 60 days record\n",
    "# thus we merge the original values from the two datasets\n",
    "dataset_merged = pd.concat((dataset_train['Open'], dataset_test['Open']), axis = 0)\n",
    "# for vertical concatenation, we use axis = 0 and gor horizontal, we use axis = 1\n",
    "inputs = dataset_merged[len(dataset_train) - len(dataset_test) - 60 : ].values\n",
    "inputs = inputs.reshape(-1, 1)\n",
    "\n",
    "# scaling the inputs; since fitting has already been done, we will go for just transform\n",
    "inputs = sc.transform(inputs)\n",
    "# making the 3D structure for prediction\n",
    "\n",
    "# forming memory for test set\n",
    "X_test = []\n",
    "for i in range(60, 80): # we go till 80 because the number of records in test are 20\n",
    "    X_test.append(inputs[i - 60 : i, 0]) # 0 means the column 0\n",
    "X_test= np.array(X_test)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# making predictions\n",
    "pred_test_set = regressor.predict((X_test))\n",
    "# inversing the scaling to get the actual format\n",
    "pred_test_set = sc.inverse_transform(pred_test_set)\n",
    "\n",
    "# plotting the visualizations\n",
    "plt.plot(real_test_set, color = 'red', label = 'Real Stock Prices from Jan 2017')\n",
    "plt.plot(pred_test_set, color = 'blue', label = 'Predicted Stock Prices from Jan 2017')\n",
    "plt.title(\"Stock Price Prediction\")\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
